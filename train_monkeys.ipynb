{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Squirrel Monkey Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of a HOST of deprecation warnings for Matterport\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# and Tensorflow\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mrcnn.model import log\n",
    "from mrcnn import visualize\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import utils\n",
    "from mrcnn.config import Config\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from termcolor import colored\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"./Mask_RCNN\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# COCO_MODEL_PATH = \"C:\\\\Users\\\\addis\\\\Documents\\\\mask_rcnn_coco.h5\"\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.3\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check tf version\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonkeysConfig(Config):\n",
    "    #################### BASE CONFIGURATION ####################\n",
    "    NAME = \"monkeys\"\n",
    "\n",
    "    # Train on 1 GPU\n",
    "    GPU_COUNT = 1\n",
    "    # batch size of 2\n",
    "    IMAGES_PER_GPU = 4\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 1 monkey\n",
    "    DETECTION_MAX_INSTANCES = 1  # we're only looking for the most prominent individual in each image\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # RPN ANCHOR SCALES left as default (32, 64, 128, 256, 512), in line with the FaterRCNN paper\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_SHAPE = np.array([512, 512, 3])\n",
    "\n",
    "    # (the number of batch iterations before a training epoch is considered finished). As we want to train on the full dataset, it's equal to num_samples/batch_size\n",
    "    # STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # VALIDATION_STEPS is similiar to STEPS_PER_EPOCH\n",
    "\n",
    "\n",
    "class InferenceConfig(MonkeysConfig):\n",
    "    GPU_COUNT = 1\n",
    "    # Batch size of 2\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "\n",
    "\n",
    "def get_config(learning_rate, detection_nms_threshold, detection_min_confidence, num_samples):\n",
    "    # config used for both training and inference\n",
    "    train_config = MonkeysConfig()\n",
    "    inference_config = InferenceConfig()\n",
    "\n",
    "    for config in [train_config, inference_config]:\n",
    "        #################### HYPERPARAMETERS TO TUNE ####################\n",
    "        config.LEARNING_RATE = learning_rate\n",
    "\n",
    "        config.DETECTION_NMS_THRESHOLD = detection_nms_threshold\n",
    "\n",
    "        config.DETECTION_MIN_CONFIDENCE = detection_min_confidence\n",
    "\n",
    "        config.STEPS_PER_EPOCH = int(num_samples / config.BATCH_SIZE)\n",
    "\n",
    "    train_config.display()\n",
    "\n",
    "    return train_config, inference_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Handles loading images and masks for the custom dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "MONKEY_CLASS_ID_STR = \"monkey\"\n",
    "\n",
    "\n",
    "class MonkeysDataset(utils.Dataset):\n",
    "    def load_monkeys(self, dataset_dir, subset):\n",
    "\n",
    "        # Add classes\n",
    "        self.add_class(MONKEY_CLASS_ID_STR, 1, MONKEY_CLASS_ID_STR)\n",
    "\n",
    "        num_images_added = 0\n",
    "       # Load annotations\n",
    "        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'polygon'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        # We mostly care about the x and y coordinates of each region\n",
    "        # Note: In VIA 2.0, regions was changed from a dict to a list.\n",
    "        annotations = json.load(\n",
    "            open(os.path.join(dataset_dir, \"via_region_data.json\")))\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            if a['filename'] in subset:\n",
    "                # Get the x, y coordinaets of points of the polygons that make up\n",
    "                # the outline of each object instance. These are stores in the\n",
    "                # shape_attributes (see json format above)\n",
    "                # The if condition is needed to support VIA versions 1.x and 2.x.\n",
    "                if type(a['regions']) is dict:\n",
    "                    polygons = [r['shape_attributes']\n",
    "                                for r in a['regions'].values()]\n",
    "                else:\n",
    "                    polygons = [r['shape_attributes'] for r in a['regions']]\n",
    "\n",
    "                # load_mask() needs the image size to convert polygons to masks.\n",
    "                # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "                # the image. This is only managable since the dataset is tiny.\n",
    "                image_path = os.path.join(dataset_dir, \"images\", a['filename'])\n",
    "                image = skimage.io.imread(image_path)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                self.add_image(\n",
    "                    MONKEY_CLASS_ID_STR,\n",
    "                    image_id=a['filename'],  # use file name as a unique image id\n",
    "                    path=image_path,\n",
    "                    width=width, height=height,\n",
    "                    polygons=polygons)\n",
    "\n",
    "                num_images_added += 1\n",
    "                print(colored(f\"Loading images {num_images_added}/{len(annotations)}\"), end='\\r')\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a monkey dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != MONKEY_CLASS_ID_STR:\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == MONKEY_CLASS_ID_STR:\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "\n",
    "Ensure the dataset is in the following form:\n",
    "\n",
    "dirName  \n",
    "└── train  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── a.jpg  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── b.jpg  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── c.jpg  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── via_region_data.json  \n",
    "└── val  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── c.jpg  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── d.jpg  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── e.jpg  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── via_regon_data.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMG_1718_00108006.jpg', 'IMG_1718_00108010.jpg', 'IMG_1718_00108017.jpg']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "DATASET_DIR = Path(\"F:/Adam/Pictures/AucklandZooImages/cv set (240)\")\n",
    "\n",
    "path = (DATASET_DIR / \"images\" / \"*.*\")\n",
    "\n",
    "X = [os.path.basename(x) for x in glob.glob(str(path))]\n",
    "print(X[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_train_dataset(X_train_fold):\n",
    "    print(\"Preparing Dataset\")\n",
    "    # Annotated using: https://www.robots.ox.ac.uk/~vgg/software/via/\n",
    "    dataset = MonkeysDataset()\n",
    "    dataset.load_monkeys(DATASET_DIR, subset=X_train_fold)\n",
    "    dataset.prepare()\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_model(model_configuration):\n",
    "    print(\"Preparing training model...\")\n",
    "\n",
    "    # Create model in training mode\n",
    "    model = modellib.MaskRCNN(mode=\"training\", config=model_configuration,\n",
    "                              model_dir=MODEL_DIR)\n",
    "\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_train, dataset_val, train_config, training_epochs, fine_tune_epochs):\n",
    "\n",
    "    model = prepare_train_model(train_config)\n",
    "\n",
    "    print(colored(\"*\"*30 + \"Training Model\" + \"*\"*30, 'green'))\n",
    "    \n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\"*30, \" Training model head layers \", \"*\"*30)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Train the head branches\n",
    "    # Passing layers=\"heads\" freezes all layers except the head\n",
    "    # layers. You can also pass a regular expression to select\n",
    "    # which layers to train by name pattern.\n",
    "\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=train_config.LEARNING_RATE,\n",
    "                epochs=training_epochs,\n",
    "                layers='heads')\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\"*30, \" Fine tune all layers \", \"*\"*30)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Fine tune all layers\n",
    "    # Passing layers=\"all\" trains all layers. You can also\n",
    "    # pass a regular expression to select which layers to\n",
    "    # train by name pattern.\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=train_config.LEARNING_RATE / 10,  # TODO determine why this is / 10\n",
    "                epochs=training_epochs+fine_tune_epochs,\n",
    "                layers=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_model(inference_config):\n",
    "    print(\"Getting inference model\")\n",
    "    # Recreate the model in inference mode\n",
    "    model = modellib.MaskRCNN(mode=\"inference\",\n",
    "                              config=inference_config,\n",
    "                              model_dir=MODEL_DIR)\n",
    "\n",
    "    # Get path to saved weights\n",
    "    # Either set a specific path or find last trained weights\n",
    "    # model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "    model_path = model.find_last()\n",
    "\n",
    "    # Load trained weights\n",
    "    print(\"Loading weights from \", model_path)\n",
    "    model.load_weights(model_path, by_name=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(dataset_val, inference_config):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\"*30, \" Evaluating model \", \"*\"*30)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Compute VOC-Style mAP @ IoU=0.5\n",
    "    # Running on 10 images. Increase for better accuracy.\n",
    "    # image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "\n",
    "    image_ids = dataset_val.image_ids\n",
    "\n",
    "    model = get_inference_model(inference_config)\n",
    "\n",
    "    APs = []\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        print(f\"Evaluating image {i}/{len(image_ids)}\", end='\\r')\n",
    "\n",
    "        # Load image and ground truth data\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset_val, inference_config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        r = results[0]\n",
    "        # Compute AP\n",
    "        AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                             r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold = IOU_THRESHOLD)\n",
    "        APs.append(AP)\n",
    "\n",
    "    print(\"Finished evaluating\")\n",
    "    return np.mean(APs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def kfold_model(n_splits, X_train, model_config, inference_config, train_epochs, fine_tune_epochs):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "    mAPs = []\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train = np.array(X_train)\n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "\n",
    "        print(colored(\"*\"*30 + f\" Beginning fold {count} \" + \"*\"*30, 'green'))\n",
    "        count += 1\n",
    "\n",
    "        # Split our training data further into a train and validation set that will be used during *Training*\n",
    "        X_train_train_sub, X_train_val_sub = train_test_split(X_train_fold, test_size=0.1, random_state=SEED)\n",
    "\n",
    "        # Train and validation sets used during model training\n",
    "        dataset_train = prepare_model_train_dataset(X_train_train_sub)\n",
    "        dataset_val = prepare_model_train_dataset(X_train_val_sub)\n",
    "\n",
    "        # Test set used to evaluate model performance *Testing*\n",
    "        dataset_test = prepare_model_train_dataset(X_test_fold)\n",
    "\n",
    "        train_model(dataset_train, dataset_val, model_config, train_epochs, fine_tune_epochs)\n",
    "\n",
    "        # Mean Average Precision for the trained model\n",
    "        mAP = evaluate_model(dataset_test, inference_config)\n",
    "\n",
    "        mAPs.append(mAP)\n",
    "\n",
    "    # Averaged mAP accross the k folds\n",
    "    averaged_mAPs = np.mean(mAPs)\n",
    "    print(colored(f\"mAP: {averaged_mAPs}\"))\n",
    "\n",
    "    return averaged_mAPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def test_hyperparameters(num_folds, X_train, train_epochs, fine_tune_epochs):\n",
    "    learning_rate_search_space = [0.01, 0.001, 0.0001]\n",
    "    detection_nms_search_space = [0.2, 0.3, 0.4]\n",
    "    detection_min_confidence_search_space = [0.7, 0.8, 0.9]\n",
    "\n",
    "    num_hyperparameters = 3\n",
    "\n",
    "    search_permutations = list(product(learning_rate_search_space, detection_nms_search_space, detection_min_confidence_search_space))\n",
    "\n",
    "    results = np.zeros((len(search_permutations), num_hyperparameters + 1))\n",
    "\n",
    "    for i, combination in enumerate(search_permutations):\n",
    "        learning_rate, detection_nms_threshold, detection_min_confidence = combination\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "        print(colored(\"*\"*30 + f\" Evaluating variation {i+1}/{len(search_permutations)} \" + \"*\"*30, \"green\"))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # Model configurations, with hyperparameters\n",
    "        train_config, inference_config = get_config(learning_rate, detection_nms_threshold, detection_min_confidence, num_samples=len(X_train))\n",
    "\n",
    "        averaged_mAPs = kfold_model(num_folds, X_train, train_config, inference_config, train_epochs, fine_tune_epochs)\n",
    "\n",
    "        results[i, :] = learning_rate, detection_nms_threshold, detection_min_confidence, averaged_mAPs\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: 50\n",
      "Reservered X_val length: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 123\n",
    "\n",
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=SEED)\n",
    "\n",
    "print(f\"X_train length: {len(X_train)}\")\n",
    "print(f\"Reservered X_val length: {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[32m****************************** Evaluating variation 1/27 ******************************\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     4\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        1\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.2\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.01\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           monkeys\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                12\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "\u001b[32m****************************** Beginning fold 1 ******************************\u001b[0m\n",
      "Preparing Dataset\n",
      "Preparing Dataset\n",
      "Preparing Dataset\n",
      "Preparing training model...\n",
      "\u001b[32m******************************Training Model******************************\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "******************************  Training model head layers  ******************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Starting at epoch 0. LR=0.01\n",
      "\n",
      "Checkpoint Path: e:\\Uni\\CompSci 760\\AucklandZooSquirrelMonkeyCOCO\\Mask_RCNN\\logs\\monkeys20211020T1742\\mask_rcnn_monkeys_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Epoch 1/1\n",
      "12/12 [==============================] - 62s 5s/step - loss: 2.3232 - rpn_class_loss: 0.0121 - rpn_bbox_loss: 0.6402 - mrcnn_class_loss: 0.0931 - mrcnn_bbox_loss: 0.9109 - mrcnn_mask_loss: 0.6670 - val_loss: 1.8495 - val_rpn_class_loss: 0.0058 - val_rpn_bbox_loss: 0.3699 - val_mrcnn_class_loss: 0.0578 - val_mrcnn_bbox_loss: 0.8544 - val_mrcnn_mask_loss: 0.5617\n",
      "\n",
      "\n",
      "\n",
      "******************************  Fine tune all layers  ******************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Starting at epoch 1. LR=0.001\n",
      "\n",
      "Checkpoint Path: e:\\Uni\\CompSci 760\\AucklandZooSquirrelMonkeyCOCO\\Mask_RCNN\\logs\\monkeys20211020T1742\\mask_rcnn_monkeys_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 87s 7s/step - loss: 0.9961 - rpn_class_loss: 0.0093 - rpn_bbox_loss: 0.2847 - mrcnn_class_loss: 0.0272 - mrcnn_bbox_loss: 0.3409 - mrcnn_mask_loss: 0.3341 - val_loss: 1.3931 - val_rpn_class_loss: 0.0069 - val_rpn_bbox_loss: 0.3222 - val_mrcnn_class_loss: 0.0516 - val_mrcnn_bbox_loss: 0.5028 - val_mrcnn_mask_loss: 0.5094\n",
      "\n",
      "\n",
      "\n",
      "******************************  Evaluating model  ******************************\n",
      "\n",
      "\n",
      "\n",
      "Getting inference model\n",
      "Loading weights from  e:\\Uni\\CompSci 760\\AucklandZooSquirrelMonkeyCOCO\\Mask_RCNN\\logs\\monkeys20211020T1742\\mask_rcnn_monkeys_0002.h5\n",
      "Re-starting from epoch 2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "len(images) must be equal to BATCH_SIZE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18208/349191307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_fine_tune_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_hyperparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_fine_tune_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18208/2898665016.py\u001b[0m in \u001b[0;36mtest_hyperparameters\u001b[1;34m(num_folds, X_train, train_epochs, fine_tune_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtrain_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_nms_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_min_confidence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0maveraged_mAPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfine_tune_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_nms_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_min_confidence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maveraged_mAPs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18208/3384082018.py\u001b[0m in \u001b[0;36mkfold_model\u001b[1;34m(n_splits, X_train, model_config, inference_config, train_epochs, fine_tune_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Mean Average Precision for the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mmAP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mmAPs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18208/1072910726.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(dataset_val, inference_config)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mmolded_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodellib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmold_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Run object detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# Compute AP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Conda\\envs\\MaskRCNN\\lib\\site-packages\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, images, verbose)\u001b[0m\n\u001b[0;32m   2493\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"inference\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Create model in inference mode.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2494\u001b[0m         assert len(\n\u001b[1;32m-> 2495\u001b[1;33m             images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\"\n\u001b[0m\u001b[0;32m   2496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: len(images) must be equal to BATCH_SIZE"
     ]
    }
   ],
   "source": [
    "num_folds = 2\n",
    "num_train_epochs = 1\n",
    "num_fine_tune_epochs = 1\n",
    "\n",
    "results = test_hyperparameters(num_folds, X_train, num_train_epochs, num_fine_tune_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7643b6ca01263e07ff32623fdc69e42a4e339236c5f6bad1bd062f88f226e188"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('MaskRCNN': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
